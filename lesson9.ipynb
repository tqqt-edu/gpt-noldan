{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "\n",
    "* Har bir belgini token deb qarashning o'rniga\n",
    "* Maxsus tokenizer dan foydalanamiz\n",
    "* Tokenizer nima - [BPE algoritm](https://en.wikipedia.org/wiki/Byte_pair_encoding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE tokenizer\n",
    "\n",
    "Quyidagi harflar ketma-ketligi berilgan deb tassavur qilaylik:\n",
    "\n",
    "aaabdaaabac\n",
    "\n",
    "Unda eng ko'p uchragan juftlik `aa` hisoblanadi. Bu juftlikni `Z` bilan almashtiraylik:\n",
    "\n",
    "ZabdZabac\n",
    "Z=aa\n",
    "\n",
    "Keyingi keng ko'p takrorlangan juftlik `ab` ni `Y` bilan almashtiraylik:\n",
    "\n",
    "ZYdZYac\n",
    "Y=ab\n",
    "Z=aa\n",
    "\n",
    "Ushbu almashtirishlardan so'ng biz boshqa ushbu `aaabdaaabac` belgilar ketma-ketligini boshqa almashtira olmaymiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tqqt1/miniconda3/envs/ai-courses/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import (Tokenizer, \n",
    "                        models, \n",
    "                        pre_tokenizers, \n",
    "                        decoders, \n",
    "                        trainers,\n",
    "                        processors,\n",
    "                        normalizers)\n",
    "import datasets\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "# Customize pre-tokenization and decoding\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(\n",
    "    add_prefix_space=True)\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)\n",
    "\n",
    "# And then train\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=20000,\n",
    "    min_frequency=2,\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()\n",
    ")\n",
    "tokenizer.train([\n",
    "    \"./shaytonat 1-3-train.txt\",\n",
    "], trainer=trainer)\n",
    "\n",
    "# And Save it\n",
    "tokenizer.save(\"shaytonat-token.json\", \n",
    "               pretty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer.from_file(\"./shaytonat-token.json\")\n",
    "\n",
    "print(tokenizer.get_vocab_size())\n",
    "\n",
    "encoded = tokenizer.encode(\"У жойига ётди. Аввалига ёлғизликдан бир оз қўрқди. Сўнг ухлаб қолди. Бу сафар ошқозони таталаб уйғонди.\")\n",
    "len(encoded.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' У жойига ётди. Аввалига ёлғизликдан бир оз қўрқди. Сўнг ухлаб қолди. Бу сафар ошқозони таталаб уйғонди.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizerni qonunlar to'plamida o'rgatish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.normalizer = normalizers.NFKC()\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=20000,\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n",
    ")\n",
    "\n",
    "tokenizer.train(list(glob.glob('/home/tqqt1/AI/projects/gpts/training/data/lexuz/train/*.txt')), trainer=trainer)\n",
    "tokenizer.save(\"lexuz-token.json\", \n",
    "               pretty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fayllar soni:  7774\n",
      "Fayllar soni:  864\n"
     ]
    }
   ],
   "source": [
    "print('Fayllar soni: ', len(list(glob.glob('/home/tqqt1/AI/projects/gpts/training/data/lexuz/train/*.txt'))))\n",
    "print('Fayllar soni: ', len(list(glob.glob('/home/tqqt1/AI/projects/gpts/training/data/lexuz/val/*.txt'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "\n",
    "    def __init__(self,\n",
    "                 block_size,\n",
    "                 batch_size,\n",
    "                 data_dir,\n",
    "                 tokenizer,\n",
    "                 drop_last=True,\n",
    "                 shuffle=True):\n",
    "        \n",
    "        self.block_size = block_size\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        files = list(glob.glob(f'{data_dir}/*.txt'))\n",
    "        self.tokens = []\n",
    "        for file_path in files:\n",
    "            with open(file_path, 'r') as f:\n",
    "                text = ' '.join(f.readlines())\n",
    "                self.tokens.append(tokenizer.encode(text).ids)\n",
    "        self.tokens = np.concatenate(self.tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return (self.tokens.shape[0] - self.block_size - 1) // self.batch_size + 0 if self.drop_last  else 1\n",
    "\n",
    "    def __iter__(self):\n",
    "        indices = np.arange(self.tokens.shape[0])\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        \n",
    "        content_size = self.block_size * self.batch_size\n",
    "\n",
    "        for batch_idx in range(len(self)):\n",
    "            xb = self.tokens[batch_idx:batch_idx+content_size]\n",
    "            yb = self.tokens[batch_idx+1:batch_idx+content_size+1]\n",
    "\n",
    "            xb = np.reshape(xb, (self.batch_size, -1))\n",
    "            yb = np.reshape(yb, (self.batch_size, -1))\n",
    "\n",
    "            yield xb, yb\n",
    "\n",
    "data_dir = '/home/tqqt1/AI/projects/gpts/training/data/lexuz/val'\n",
    "\n",
    "val_loader = DataLoader(block_size=128,\n",
    "                        batch_size=32,\n",
    "                        data_dir=data_dir,\n",
    "                        tokenizer=tokenizer)\n",
    "xb, yb = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizerni O'zbek tilidagi kitoblar to'plamida o'rgatish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\n",
    "    \"tahrirchi/uz-books\", \n",
    "    split=\"lat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator(batch_size=1000):\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i : i + batch_size][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.normalizer = normalizers.NFKC()\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "trainer = trainers.UnigramTrainer(\n",
    "    vocab_size=20000,\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n",
    ")\n",
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT\n",
    "\n",
    "1. Pre-training -> Training - unsupervised  <-> BERT -> Harajat\n",
    "\n",
    "2. Fine-tuning: kiruvchi matn <-> chiquvchi matn -> Supervised -> LoRA -> low-rank adaptation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "64\n",
    "32\n",
    "16\n",
    "8\n",
    "4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = 3000*10000\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = 3000*10\n",
    "B = 10*10000\n",
    "A+B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A*B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Tarjimon"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-courses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
