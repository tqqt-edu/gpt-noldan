{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. belgini -> vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beliglar soni: 93\n",
      "O'rgatuvchida:  1848219\n",
      "Sinovda:  205358\n"
     ]
    }
   ],
   "source": [
    "with open('shaytonat 1-3.txt', 'r') as f:\n",
    "  text = f.read()\n",
    "bag = list(set(text))\n",
    "n_bag = len(bag)\n",
    "print(f'Beliglar soni: {n_bag}')\n",
    "\n",
    "n_data = len(text)\n",
    "val_size = 0.1\n",
    "n_train = int((1 - val_size) * n_data)\n",
    "n_val = n_data - n_train\n",
    "\n",
    "train_data = text[:n_train]\n",
    "val_data = text[n_train:]\n",
    "print(\"O'rgatuvchida: \", n_train)\n",
    "print(\"Sinovda: \", n_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda s: [bag.index(l) for l in s]\n",
    "decode = lambda ids: \"\".join([bag[id] for id in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7482, -0.8349,  1.0895, -0.0689, -0.4024],\n",
       "         [ 1.1198,  1.3322, -0.0992,  0.1752,  0.2369],\n",
       "         [-0.8413,  0.4223, -0.9354, -0.4959, -0.0095]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (1, N)\n",
    "a = torch.Tensor([encode('Сиз')]).to(torch.int32)\n",
    "# lookup table\n",
    "emb_layer = nn.Embedding(n_bag, 5)\n",
    "# (1, N, 10)\n",
    "a_emb = emb_layer(a)\n",
    "a_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShDataset(data.Dataset):\n",
    "\n",
    "  def __init__(self, text, T):\n",
    "    self.text = text\n",
    "    self.T = T\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.text) - self.T - 1\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return np.array(encode(self.text[idx:idx+self.T])), bag.index(self.text[idx+self.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 50\n",
    "train_dataset = ShDataset(train_data, T)\n",
    "val_dataset = ShDataset(val_data, T)\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, \n",
    "                               batch_size=256, \n",
    "                               shuffle=True, \n",
    "                               drop_last=True,\n",
    "                               num_workers=12,\n",
    "                               pin_memory=True)\n",
    "val_loader = data.DataLoader(val_dataset, \n",
    "                             batch_size=256, \n",
    "                             shuffle=False, \n",
    "                             drop_last=False,\n",
    "                             num_workers=12,\n",
    "                             pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size=93,\n",
    "                 emb_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_layer = nn.Embedding(vocab_size,\n",
    "                                      emb_dim)\n",
    "        self.act_layer = nn.LeakyReLU()\n",
    "        self.model = nn.Sequential(*[\n",
    "            nn.Linear(emb_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, vocab_size),\n",
    "        ])\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        # (batch_size, block_size)\n",
    "        # idx\n",
    "        # (batch_size, block_size, emb_dim)\n",
    "        idx_emb = self.act_layer(self.emb_layer(idx))\n",
    "        idx_emb = torch.mean(idx_emb, dim=1)\n",
    "        logits = self.model(idx_emb)\n",
    "\n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LLM(vocab_size=n_bag, emb_dim=256).to(device)\n",
    "loss_module = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=5e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "  train_loss = 0\n",
    "  train_size = 0\n",
    "  step = 0\n",
    "  for texts, letters in train_loader:\n",
    "    step += 1\n",
    "    texts = torch.Tensor(texts).to(device)\n",
    "    letters = torch.Tensor(letters).to(dtype=torch.long).to(device)\n",
    "    train_size += texts.shape[0]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    preds = model(texts)\n",
    "    loss = loss_module(preds, letters)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_loss += loss.item()\n",
    "\n",
    "  train_loss /= step\n",
    "\n",
    "  val_loss = 0\n",
    "  val_size = 0\n",
    "  step = 0\n",
    "\n",
    "  for texts, letters in val_loader:\n",
    "    step += 1\n",
    "    texts = torch.Tensor(texts).to(device)\n",
    "    letters = torch.Tensor(letters).to(dtype=torch.long).to(device)\n",
    "    val_size += texts.shape[0]\n",
    "\n",
    "    preds = model(texts)\n",
    "    loss = loss_module(preds, letters)\n",
    "\n",
    "    val_loss += loss.item()\n",
    "\n",
    "  val_loss /= step\n",
    "\n",
    "  print(f\"Epoch: {epoch+1}/{epochs}, loss: {train_loss:.4f}, val loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def text_generate(model: nn.Module,\n",
    "                  instruction: str,\n",
    "                  size: int=200):\n",
    "  \n",
    "  model.eval()\n",
    "  ins = instruction\n",
    "  print(ins)\n",
    "  for i in range(size):\n",
    "    current_ids = np.array(encode(ins))[np.newaxis, :]\n",
    "    # (batch_size, T)\n",
    "    current_ids_tensor = torch.Tensor(current_ids).to(torch.int32)\n",
    "    current_ids_tensor = current_ids_tensor.to(device)\n",
    "    probs_next_letters = model(current_ids_tensor)\n",
    "    probs_next_letters = torch.exp(probs_next_letters).cpu().numpy()[0]\n",
    "    probs_next_letters /= sum(probs_next_letters)\n",
    "    next_letter_id = np.argmax(np.random.multinomial(1, probs_next_letters))\n",
    "    print(bag[next_letter_id], end='')\n",
    "    ins += bag[next_letter_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_generate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ins_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mУ сесканиб\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtext_generate\u001b[49m(model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[1;32m      3\u001b[0m               instruction\u001b[38;5;241m=\u001b[39mins_text,\n\u001b[1;32m      4\u001b[0m               size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_generate' is not defined"
     ]
    }
   ],
   "source": [
    "ins_text = \"У сесканиб\"\n",
    "text_generate(model=model, \n",
    "              instruction=ins_text,\n",
    "              size=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-courses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
